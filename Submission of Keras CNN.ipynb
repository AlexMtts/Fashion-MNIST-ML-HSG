{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Submission of Keras CNN.ipynb","provenance":[{"file_id":"1sUf7O2dxXs-wlfiAVGzRsoU__o0GWItk","timestamp":1589663142825}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNvNP9xUHbI32BYetMntY1M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"p53UJ35MrqJm","colab_type":"text"},"source":["**Convolutional Neural Network:**\n","\n","BjÃ¶rn Leon Neumann (17-619-586)\n","\n","Alexander Martin Mattes (17-619-529)\n","\n","\n","**Accuracy rate (evaluation data)**: 0.9501000046730042\n","\n","**Accuracy rate (training data)**: ~0.992\n","\n","The accuracy rate on the training data differs slightly whenever the code is run, as it is the accuracy resulting from going through 1 more epoch starting from the weights that result in an evaluation accuracy of 95.01%\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SPNsdgP9awPH","colab_type":"text"},"source":["**Instructions (IMPORTANT):**\n","\n","If you want to load the weights that result in a test accuracy of 95.01%, set the variable **load_existing_weights** to **True**. \n","\n","If you set **load_weights_from_remote** to **False**, keep in mind, that you are required to have the cp-0200.ckpt file in a folder called **weights** in order to make this work. Thus, you have to create a folder called **weights** and put the following file into it: https://drive.google.com/open?id=19p_AsT2WVSdpvtmm-Uv3q51SD9lSqxXb\n","\n","However, if you leave **load_weights_from_remote** to **True**, the weights will be loaded in raw form from https://drive.google.com/uc?id=19p_AsT2WVSdpvtmm-Uv3q51SD9lSqxXb\n","\n","If you want to train the model from scratch, set the variable **load_existing_weights** to **False**\n","\n","If you want to use data augmentation, set the variable **data_augmentation** to **True**. Otherwise set it to **False**."]},{"cell_type":"markdown","metadata":{"id":"jplt1h2s8fHL","colab_type":"text"},"source":["# 0. Set your initial hyperparameters"]},{"cell_type":"code","metadata":{"id":"scj-Qj3zwFW9","colab_type":"code","colab":{}},"source":["load_existing_weights = False # CHANGE TO FALSE if you want to train the model from scratch\n","data_augmentation = True\n","\n","load_weights_from_remote = True\n","weights_file_url = 'https://drive.google.com/uc?id=19p_AsT2WVSdpvtmm-Uv3q51SD9lSqxXb'\n","\n","\n","batch_size = 256\n","num_classes = 10\n","epochs = 200\n","\n","learning_rate = 0.001\n","\n","\n","data_augmentation_params = {\n","  \"rotation_range\": 10,      # randomly rotate images in the range (degrees, 0 to 180) (WAS at 10)\n","  \"width_shift_range\": 0.1,  # randomly shift images horizontally (fraction of total width)\n","  \"height_shift_range\": 0.1, # randomly shift images vertically (fraction of total height)\n","  \"shear_range\": 0.1,        # set range for random shear\n","  \"horizontal_flip\": True  # randomly flip images\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vzPeD6b68AZZ","colab_type":"text"},"source":["# 1. Imports and Setup"]},{"cell_type":"code","metadata":{"id":"4OmdSeWGW6dk","colab_type":"code","outputId":"6b8b6d36-a118-4efb-e490-2b0015a8558b","executionInfo":{"status":"ok","timestamp":1590041144960,"user_tz":-120,"elapsed":8946,"user":{"displayName":"Leon New","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7kNJv1dshlx69jUHBRQZ7eT65yQAFcNJzuwjrkw=s64","userId":"02061277165564939938"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["!pip install pyyaml h5py  # Required to save models in HDF5 format\n","import os\n","\n","from __future__ import print_function\n","import keras\n","from keras.datasets import fashion_mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K\n","\n","import numpy as np\n","np.random.seed(1337) # To have consistency\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from keras.utils.data_utils import get_file\n","from keras.utils.vis_utils import plot_model\n","\n","\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","\n","if K.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n","    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mTPZrd659J2s","colab_type":"text"},"source":["# 2. Setting up Model"]},{"cell_type":"code","metadata":{"id":"-K2M48dr9NPc","colab_type":"code","outputId":"a07434ab-7dc8-449e-8eac-11d197994ec4","executionInfo":{"status":"ok","timestamp":1590041154474,"user_tz":-120,"elapsed":18448,"user":{"displayName":"Leon New","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7kNJv1dshlx69jUHBRQZ7eT65yQAFcNJzuwjrkw=s64","userId":"02061277165564939938"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = Sequential()\n","\n","## Progress\n","# Went from 2 layers to 3\n","# Added BatchNormalization\n","# Went from 3 to 4 convolutional layers\n","# Removed the MaxPooling from the 1st and 3rd layer\n","# Went from 32-64-128-256 to 64-128-256-512 -> worse results\n","# Went back to 32-64-128-256 and increased the batch size from 256 to 512 -> converged slower (eventually slightly worse results)\n","# Set learning rate to 0.0001 (converged slower) -> worse results\n","\n","# Training_1: rotation 90 degrees; last fully connected layer 256\n","# Training_2: rotation 90 degrees; last fully connected layer 512\n","# Training_3: rotation 10 degrees; last fully connected layer 512\n","\n","### Changed hperparameters summary:\n","## Initial model:\n","# Batch size\n","# Number of epochs\n","# Number of layers\n","# Neuron output size\n","# Different Activation functions\n","# Different Optimizers\n","# Kernzel Size\n","# Pool Size\n","# Learning rate\n","## Additional hyperparameters (data augmentation):\n","# Rotation range\n","# Width shift range\n","# Height shift range\n","# Shear range\n","# Horizontal flip\n","\n","\n","model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n","model.add(BatchNormalization())\n","model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","\n","model.add(Flatten())\n","model.add(Dense(512, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.5))\n","model.add(Dense(512, activation='relu')) #256 before\n","model.add(BatchNormalization())\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","print(model.summary())\n","#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n","              metrics=['accuracy'])\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 28, 28, 32)        320       \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 28, 28, 32)        128       \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 28, 28, 32)        128       \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 28, 28, 32)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 28, 28, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 14, 14, 128)       73856     \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 14, 14, 128)       512       \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 14, 14, 128)       147584    \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 14, 14, 128)       512       \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 14, 14, 256)       295168    \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 14, 14, 256)       1024      \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 14, 14, 256)       590080    \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 14, 14, 256)       1024      \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 7, 7, 256)         0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 7, 7, 256)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 12544)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               6423040   \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 512)               262656    \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 512)               2048      \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 512)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 10)                5130      \n","=================================================================\n","Total params: 7,870,442\n","Trainable params: 7,866,474\n","Non-trainable params: 3,968\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"baeCjxlF-Xdr","colab_type":"text"},"source":["# 3. Model Training"]},{"cell_type":"code","metadata":{"id":"AKcDr8wt-aAl","colab_type":"code","outputId":"43953dfd-28f1-4a18-c9ab-bc0984f52621","executionInfo":{"status":"ok","timestamp":1590041165440,"user_tz":-120,"elapsed":29405,"user":{"displayName":"Leon New","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7kNJv1dshlx69jUHBRQZ7eT65yQAFcNJzuwjrkw=s64","userId":"02061277165564939938"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["if not load_existing_weights:\n","\n","  # The following saving weights functionality has been commented out for submission purposes\n","\n","  # Save weights\n","  ###checkpoint_path = \"training_1/cp-{epoch:04d}.ckpt\"\n","  ###checkpoint_dir = os.path.dirname(checkpoint_path)\n","  # Create a callback that saves the model's weights\n","  ###cp_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","  ###                                                save_weights_only=True,\n","  ###                                                verbose=1)\n","\n","  if not data_augmentation:\n","\n","      print('Not using data augmentation.')\n","      \n","      model.fit(x_train, y_train,\n","            batch_size=batch_size,\n","            epochs=epochs,\n","            verbose=1,\n","            validation_data=(x_test, y_test))\n","            ####callbacks=[cp_callback]) # Not used as part of testing\n","      \n","  else:\n","\n","      print('Using real-time data augmentation.')\n","      # This will do preprocessing and realtime data augmentation:\n","      datagen = ImageDataGenerator(\n","          rotation_range=data_augmentation_params[\"rotation_range\"],\n","          width_shift_range=data_augmentation_params[\"width_shift_range\"],\n","          height_shift_range=data_augmentation_params[\"height_shift_range\"],\n","          shear_range=data_augmentation_params[\"shear_range\"],\n","          horizontal_flip=data_augmentation_params[\"horizontal_flip\"])\n","      \n","      # Compute quantities required for feature-wise normalization\n","      # (std, mean, and principal components if ZCA whitening is applied).\n","      datagen.fit(x_train)\n","\n","      # Fit the model on the batches generated by datagen.flow().\n","      model.fit_generator(datagen.flow(x_train, y_train,\n","                                      batch_size=batch_size),\n","                          epochs=epochs,\n","                          validation_data=(x_test, y_test),\n","                          workers=4)\n","                          ####callbacks=[cp_callback]) # Not used as part of testing\n","\n","  #This creates a single collection of TensorFlow checkpoint files that are updated at the end of each epoch\n","  !ls {checkpoint_dir}\n","\n","else:\n","\n","  if not load_weights_from_remote:\n","    checkpoint_path = \"weights/cp-0200.ckpt\"\n","  else:\n","    checkpoint_path = get_file(\n","              'our_weights',\n","              weights_file_url)\n","    \n","  # Loads the weights\n","  model.load_weights(checkpoint_path)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading data from https://drive.google.com/uc?id=19p_AsT2WVSdpvtmm-Uv3q51SD9lSqxXb\n","31563776/Unknown - 4s 0us/step"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5CDk6r5n-fsF","colab_type":"text"},"source":["# 4. Results"]},{"cell_type":"code","metadata":{"id":"gBqovAyn-i73","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"23e2b983-428a-4b5e-a08e-a4e0d0652e4c","executionInfo":{"status":"ok","timestamp":1590041172263,"user_tz":-120,"elapsed":36224,"user":{"displayName":"Leon New","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7kNJv1dshlx69jUHBRQZ7eT65yQAFcNJzuwjrkw=s64","userId":"02061277165564939938"}}},"source":["score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Test loss: 0.21510290865816642\n","Test accuracy: 0.9501000046730042\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RRE1qrvJ7ffR","colab_type":"text"},"source":["Showing additional information, including **accuracy rate on the training data** by going through one more epoch starting from the weights that achieve 95.01% evaluation accuracy\n","\n","Even though this is the results section, here we **train** our model again as we go through one more epoch to get results for the training."]},{"cell_type":"code","metadata":{"id":"TRwvXoVn7vaK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"eedc8a73-4dda-4080-9834-68cc2bbaf08e"},"source":["# Showing additional information, including accuracy rate on the training data \n","# by going through one more epoch based on the weights that achieve 95.01% evaluation accuracy\n","\n","if data_augmentation:\n","  print('# Fit model on training data')\n","  history = model.fit(x_train, y_train,\n","                      batch_size=256,\n","                      epochs=1,\n","                      validation_data=(x_test, y_test))\n","  print('\\nHistory dictionary showing additional information, including the accuracy rate of the training data. \\nHere we are going through one more epoch based on the weights that achieve 95.01% evaluation accuracy.\\n\\n', history.history)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["# Fit model on training data\n","Train on 60000 samples, validate on 10000 samples\n","Epoch 1/1\n","48896/60000 [=======================>......] - ETA: 8s - loss: 0.0230 - accuracy: 0.9919"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I0QFZMzWkp3b","colab_type":"text"},"source":["# 5. Confusion Matrix:"]},{"cell_type":"code","metadata":{"id":"V1aaTS9girJW","colab_type":"code","colab":{}},"source":["# Visualisation\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.pylab as pylab\n","import seaborn as sns\n","import missingno as msno\n","\n","# Handle table-like data and matrices :\n","import itertools\n","\n","# Modelling Helpers :\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=90)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","\n","# Predict the values from the validation dataset\n","Y_pred = model.predict(x_test)\n","# Convert predictions classes to one hot vectors \n","Y_pred_classes = np.argmax(Y_pred,axis = 1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(y_test,axis = 1) \n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n","# plot the confusion matrix\n","plot_confusion_matrix(confusion_mtx, classes = ['T-shirt/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'])"],"execution_count":0,"outputs":[]}]}